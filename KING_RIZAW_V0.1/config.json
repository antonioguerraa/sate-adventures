{
  "model_type": "gpt2",
  "architectures": ["GPT2LMHeadModel"],
  "vocab_size": 8000,
  "n_positions": 512,
  "n_ctx": 512,
  "n_embd": 128,
  "n_layer": 6,
  "n_head": 8,
  "bos_token_id": 0,
  "eos_token_id": 1
}